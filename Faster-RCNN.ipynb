{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a747a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "62290abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VolleyballDataset(Dataset):\n",
    "    def __init__(self, root_dir, annotation_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.annotation_dir = annotation_dir\n",
    "        self.transform = transform\n",
    "        self.video_dirs = sorted([int(d) for d in os.listdir(root_dir) if d.isdigit()])\n",
    "        self.sequence_len = 41\n",
    "        self.sequence_indices = []\n",
    "        for video_dir in self.video_dirs:\n",
    "            frames_dir = os.path.join(annotation_dir, str(video_dir))\n",
    "            frames = sorted([int(f[:-4]) for f in os.listdir(frames_dir) if f.endswith('.txt')])\n",
    "            for i in range(len(frames) - self.sequence_len + 1):\n",
    "                self.sequence_indices.append(frames[i + 20])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return 55\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        video_dir_idx = idx // len(self.sequence_indices)\n",
    "        sequence_idx = self.sequence_indices[idx % len(self.sequence_indices)]\n",
    "        video_dir = os.path.join(self.root_dir, str(self.video_dirs[video_dir_idx]))\n",
    "        sequence_file = os.path.join(self.annotation_dir, str(self.video_dirs[video_dir_idx]), str(sequence_idx)+'.txt')\n",
    "        with open(sequence_file, 'r') as f:\n",
    "            line = f.readline()\n",
    "            while line.startswith('0 0'):\n",
    "                line = f.readline()\n",
    "            x, y = [float(coord) for coord in line.split()]\n",
    "        img_dir = os.path.join(video_dir, str(sequence_idx))\n",
    "        img_files = sorted(os.listdir(img_dir))\n",
    "        img_files = [os.path.join(img_dir, f) for f in img_files]\n",
    "        img_sequence = [Image.open(f) for f in img_files]\n",
    "        if self.transform:\n",
    "            img_sequence = [self.transform(img) for img in img_sequence]\n",
    "        center_idx = len(img_sequence) // 2\n",
    "        img_tensor = img_sequence[center_idx]\n",
    "        \n",
    "        boxes = torch.as_tensor([[x-5, y-5, x+5, y+5]], dtype=torch.float32)\n",
    "        labels = torch.as_tensor([1], dtype=torch.int64)\n",
    "        image_id = torch.tensor(idx)\n",
    "        area = (boxes[0, 3] - boxes[0, 1]) * (boxes[0, 2] - boxes[0, 0])\n",
    "        is_crowd = torch.zeros((1,), dtype=torch.int64)\n",
    "        \n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"image_id\"] = image_id\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = is_crowd\n",
    "\n",
    "        return img_tensor, target\n",
    "\n",
    "\n",
    "root_dir = 'C:/Users/salba/Documents/videos'\n",
    "annotation_dir = 'C:/Users/salba/Documents/volleyball_ball_annotation'\n",
    "dataset = VolleyballDataset(root_dir, annotation_dir, transform=torchvision.transforms.ToTensor())\n",
    "\n",
    "train_videos = [1, 3, 6, 7, 10, 13, 15, 16, 18, 22, 23, 31, 32, 36, 38, 39, 40, 41, 42, 48, 50, 52, 53, 54]\n",
    "val_videos = [0, 2, 8, 12, 17, 19, 24, 26, 27, 28, 30, 33, 46, 49, 51]\n",
    "test_videos = [4, 5, 9, 11, 14, 20, 21, 25, 29, 34, 35, 37, 43, 44, 45, 47]\n",
    "\n",
    "train_idx = [i for i in range(len(dataset)) if dataset.video_dirs[i] in train_videos]\n",
    "val_idx = [i for i in range(len(dataset)) if dataset.video_dirs[i] in val_videos]\n",
    "test_idx = [i for i in range(len(dataset)) if dataset.video_dirs[i] in test_videos]\n",
    "\n",
    "train_dataset = torch.utils.data.Subset(dataset, train_idx)\n",
    "val_dataset = torch.utils.data.Subset(dataset, val_idx)\n",
    "test_dataset = torch.utils.data.Subset(dataset, test_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34bbde12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train samples: 24\n",
      "Number of val samples: 15\n",
      "Number of test samples: 16\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54]\n"
     ]
    }
   ],
   "source": [
    "print('Number of train samples:', len(train_dataset))\n",
    "print('Number of val samples:', len(val_dataset))\n",
    "print('Number of test samples:', len(test_dataset))\n",
    "print(dataset.video_dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b545cdaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0]  [ 0/12]  eta: 0:07:30  lr: 0.000459  loss: 2.3845 (2.3845)  loss_classifier: 0.5452 (0.5452)  loss_box_reg: 0.0040 (0.0040)  loss_objectness: 1.3749 (1.3749)  loss_rpn_box_reg: 0.4604 (0.4604)  time: 37.5280  data: 1.3625\n",
      "Epoch: [0]  [ 1/12]  eta: 0:06:55  lr: 0.000913  loss: 1.9470 (2.1657)  loss_classifier: 0.5452 (0.5547)  loss_box_reg: 0.0023 (0.0031)  loss_objectness: 1.0355 (1.2052)  loss_rpn_box_reg: 0.3451 (0.4027)  time: 37.7588  data: 1.3481\n",
      "Epoch: [0]  [ 2/12]  eta: 0:06:17  lr: 0.001367  loss: 1.9470 (1.9707)  loss_classifier: 0.5452 (0.5307)  loss_box_reg: 0.0030 (0.0031)  loss_objectness: 1.0355 (1.0692)  loss_rpn_box_reg: 0.3451 (0.3676)  time: 37.7980  data: 1.3743\n",
      "Epoch: [0]  [ 3/12]  eta: 0:05:37  lr: 0.001821  loss: 1.5807 (1.8214)  loss_classifier: 0.4829 (0.4784)  loss_box_reg: 0.0030 (0.0031)  loss_objectness: 0.8884 (1.0240)  loss_rpn_box_reg: 0.2974 (0.3159)  time: 37.5426  data: 1.3618\n",
      "Epoch: [0]  [ 4/12]  eta: 0:04:58  lr: 0.002275  loss: 1.5807 (1.7053)  loss_classifier: 0.4829 (0.4134)  loss_box_reg: 0.0030 (0.0030)  loss_objectness: 0.8884 (0.9941)  loss_rpn_box_reg: 0.2974 (0.2948)  time: 37.3593  data: 1.3587\n",
      "Epoch: [0]  [ 5/12]  eta: 0:04:20  lr: 0.002730  loss: 1.3736 (1.5905)  loss_classifier: 0.3214 (0.3542)  loss_box_reg: 0.0025 (0.0026)  loss_objectness: 0.8746 (0.9624)  loss_rpn_box_reg: 0.2103 (0.2713)  time: 37.2502  data: 1.3549\n",
      "Epoch: [0]  [ 6/12]  eta: 0:03:42  lr: 0.003184  loss: 1.3736 (1.4650)  loss_classifier: 0.3214 (0.3071)  loss_box_reg: 0.0025 (0.0025)  loss_objectness: 0.8746 (0.9007)  loss_rpn_box_reg: 0.2103 (0.2548)  time: 37.1420  data: 1.3478\n",
      "Epoch: [0]  [ 7/12]  eta: 0:03:05  lr: 0.003638  loss: 1.2409 (1.3751)  loss_classifier: 0.1536 (0.2707)  loss_box_reg: 0.0023 (0.0024)  loss_objectness: 0.8039 (0.8593)  loss_rpn_box_reg: 0.1609 (0.2426)  time: 37.0724  data: 1.3436\n",
      "Epoch: [0]  [ 8/12]  eta: 0:02:28  lr: 0.004092  loss: 1.2409 (1.3103)  loss_classifier: 0.1536 (0.2430)  loss_box_reg: 0.0025 (0.0025)  loss_objectness: 0.8039 (0.8301)  loss_rpn_box_reg: 0.1723 (0.2348)  time: 37.0283  data: 1.3411\n",
      "Epoch: [0]  [ 9/12]  eta: 0:01:50  lr: 0.004546  loss: 1.0165 (1.2624)  loss_classifier: 0.0578 (0.2212)  loss_box_reg: 0.0025 (0.0026)  loss_objectness: 0.7974 (0.8049)  loss_rpn_box_reg: 0.1723 (0.2337)  time: 36.9778  data: 1.3389\n",
      "Epoch: [0]  [10/12]  eta: 0:01:13  lr: 0.005000  loss: 1.0165 (1.2203)  loss_classifier: 0.0578 (0.2028)  loss_box_reg: 0.0025 (0.0026)  loss_objectness: 0.7974 (0.7705)  loss_rpn_box_reg: 0.2103 (0.2444)  time: 36.9443  data: 1.3346\n",
      "Epoch: [0]  [11/12]  eta: 0:00:36  lr: 0.005000  loss: 0.8928 (1.1930)  loss_classifier: 0.0254 (0.1877)  loss_box_reg: 0.0023 (0.0025)  loss_objectness: 0.5958 (0.7374)  loss_rpn_box_reg: 0.2103 (0.2654)  time: 36.9132  data: 1.3328\n",
      "Epoch: [0] Total time: 0:07:22 (36.9138 s / it)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'float' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 27\u001b[0m\n\u001b[0;32m     25\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m train_one_epoch(model, optimizer, train_loader, device, epoch, print_freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     26\u001b[0m lr_scheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m---> 27\u001b[0m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\Documents\\Personal\\vtracker\\engine.py:85\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(model, data_loader, device)\u001b[0m\n\u001b[0;32m     82\u001b[0m metric_logger \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mMetricLogger(delimiter\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     83\u001b[0m header \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 85\u001b[0m coco \u001b[38;5;241m=\u001b[39m \u001b[43mget_coco_api_from_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     86\u001b[0m iou_types \u001b[38;5;241m=\u001b[39m _get_iou_types(model)\n\u001b[0;32m     87\u001b[0m coco_evaluator \u001b[38;5;241m=\u001b[39m CocoEvaluator(coco, iou_types)\n",
      "File \u001b[1;32m~\\Documents\\Personal\\vtracker\\coco_utils.py:206\u001b[0m, in \u001b[0;36mget_coco_api_from_dataset\u001b[1;34m(dataset)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dataset, torchvision\u001b[38;5;241m.\u001b[39mdatasets\u001b[38;5;241m.\u001b[39mCocoDetection):\n\u001b[0;32m    205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dataset\u001b[38;5;241m.\u001b[39mcoco\n\u001b[1;32m--> 206\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconvert_to_coco_api\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\Personal\\vtracker\\coco_utils.py:182\u001b[0m, in \u001b[0;36mconvert_to_coco_api\u001b[1;34m(ds)\u001b[0m\n\u001b[0;32m    180\u001b[0m ann[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategory_id\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m labels[i]\n\u001b[0;32m    181\u001b[0m categories\u001b[38;5;241m.\u001b[39madd(labels[i])\n\u001b[1;32m--> 182\u001b[0m ann[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marea\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mareas\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    183\u001b[0m ann[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miscrowd\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m iscrowd[i]\n\u001b[0;32m    184\u001b[0m ann[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m ann_id\n",
      "\u001b[1;31mTypeError\u001b[0m: 'float' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.faster_rcnn import FasterRCNN_ResNet50_FPN_Weights\n",
    "import utils\n",
    "import torchvision.models.detection as detection\n",
    "from engine import train_one_epoch, evaluate\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT)\n",
    "\n",
    "num_classes = 2  # ball and background\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=utils.collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, collate_fn=utils.collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, collate_fn=utils.collate_fn)\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_one_epoch(model, optimizer, train_loader, device, epoch, print_freq=1)\n",
    "    lr_scheduler.step()\n",
    "    evaluate(model, val_loader, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2639bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
